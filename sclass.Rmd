
# Question 2. K-nearest neighbors

```{r include=FALSE}
 #### plot the graph
library(tidyverse)
library(FNN)
library(mosaic)
sclass = read.csv('sclass.csv')
summary(sclass)

# Focus on 2 trim levels: 350 and 65 AMG
sclass350 = subset(sclass, trim == '350')
sclass65AMG = subset(sclass, trim == '65 AMG')

```

With information of over 29,000 Mercedes-Benz S Class vehicles, our goal is to use K-nearest neighbors to build a predictive model for price, given mileage, separately for each of two trim levels: 350 and 65 AMG.

## Model for MB S350 

### Figure 2.1. 
```{r echo=FALSE, warning= FALSE}
plot350 =ggplot(data = sclass350) + 
  geom_point(mapping = aes(x = mileage, y = price), color='blue')+
  labs(title="MB S350 ", 
     x="Mileage",
     y = "Price in USD")
plot350
```

In the figure 2.1. we can see the negative relation between the mileage and  the price, as expected.

The following steep is divide our data between the test and train data for both variables. After that, we generate random samples of our subsets of data. Then, we calculate the rsme generated at each level of K in our k nearest neighbor model for price of a 350 s class given the mileage. 

### Figure 2.2. 
```{r include=FALSE, echo=FALSE, warning= FALSE}
# Make a train-test split 350
N350 = nrow(sclass350)
N_train350 = floor(0.8*N350)
N_test350 = N350 - N_train350


#####Train/test split
# randomly sample a set of data points to include in the training set
train_ind350 = sample.int(N350, N_train350, replace=FALSE)

# Define the training and testing set
D_train350 = sclass350[train_ind350,]
D_test350 = sclass350[-train_ind350,]

# optional book-keeping step:
# reorder the rows of the testing set by the mileage variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test350 = arrange(D_test350, mileage)
head(D_test350)

# Now separate the training and testing sets into features (X) and outcome (y)
X_train350 = select(D_train350, mileage)
y_train350 = select(D_train350, price)
X_test350 = select(D_test350, mileage)
y_test350 = select(D_test350, price)

#####
########## K -Nearest Neighbor Model##############
#####
# define a helper function for calculating RMSE
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}

knn_output <- data.frame(K=c(),rmse=c())

for(i in c(3:nrow(X_train350))){
  knn_Gen = knn.reg(train = X_train350, test = X_test350, y = y_train350, k=i)
  ypred_knn350 = knn_Gen$pred
  knn_rmse =rmse(y_test350,ypred_knn350)
  knn_output = rbind(knn_output,c(i,knn_rmse))
}
colnames(knn_output) <- c("k","RMSE")
k_min = knn_output$k[which.min(knn_output$RMSE)]

###########Graph rsme vs k ################
plot350_k= ggplot(data = knn_output)+
  geom_line(aes(x = k, y = RMSE))+
  geom_line(aes(x = k_min, y = RMSE), col = "red")+
  labs(title="KNN's Models for 350 Trim ", 
       x="K",
       y = "RMSE")
```
```{r echo=FALSE, warning= FALSE}
plot350_k
```
The figure 2.2. shows the series of RMSE at every value of K, the red line indicate the k which generate the minimum RMSE. In this case the k value is  `r k_min`. However this value will change any time we generate a new random sample, to decide the best k we repeated the process ten times and then we took the average of the minium k.  

### Figure 2.3. 
```{r echo=FALSE, warning= FALSE}
###The BEST model
knn_best = knn.reg(train = X_train350, test = X_test350, y = y_train350, k=k_min)
ypred_knn350_best=knn_best$pred
rmse(y_test350, ypred_knn350_best)

####
# plot the fit
####
# attach the predictions to the test data frame
D_test350$ypred_knn350_best = ypred_knn350_best

p_test350 = ggplot(data = D_test350) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey')+ 
  geom_path(aes(x = mileage, y = ypred_knn350_best), color='red')+
  labs(title="KNN model with minimum RMSE", 
       x="Mileage",
       y = "Price in USD")
 p_test350
```

After run the model with the best k, we generate the fitted values to compare with our test subset. the figure 2.3. shows the fit of the chosen model.  

## Model for MB S65AMG
