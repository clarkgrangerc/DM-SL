library(class)
library(reshape2)
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
summary(lmbase)
install.packages("xtable")
install.packages("xtable")
install.packages("xtable")
install.packages("xtable")
install.packages("xtable")
install.packages("xtable")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(class)
library(reshape2)
library(xtable)
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
xtable(lmbase)
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
xtable(lmbase)
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
ktable(lmbase)
install.packages("pander")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(class)
library(reshape2)
library(pander)
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
pander(lmbase)
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
pander.lm(lmbase)
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
pander(summary(lmbase))
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
pander(lm(lmbase))
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
pander(coef(lmbase))
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
panderOptions('table.split.table', 300)
pander(summary(lmbase), )
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
panderOptions('table.split.table', 200)
pander(summary(lmbase), )
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
panderOptions('table.split.table', 100)
pander(summary(lmbase), )
brca = read.csv("brca.csv")
model_recall = glm(recall ~ . - cancer, data=brca, family=binomial)
pander(summary(model_recall))
pander(xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3)))
xtabs(~cancer + recall + density, brca) %>% prop.table(margin=c(2, 3))
pander(xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3)))
pander(xtabs(~cancer + recall + density, brca) %>% prop.table(margin=c(2, 3)))
pander(tabular(xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3))))
pander(table(xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3))))
pander(xtabs(~cancer + recall + density, brca) %>% prop.table(margin=c(2, 3)))
pander(gtable(xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3))))
pander(stat.table(xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3))))
pander(ftable(xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3))))
pander(xtabs(~cancer + recall + density, brca) %>% prop.table(margin=c(2, 3)))
pander((xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3))))
pander(xtabs(~cancer + recall + density, brca) %>% prop.table(margin=c(2, 3)))
install.packages(c("jtools", "margins"))
colnames(knngraph) <- c("k","AVG_ACC")
colnames(knngraph) <- c("k","AVG_ACC")
colnames(knngraph) <- c("k","AVG_ACC")
colnames(knngraph) <- c("k","AVG_ACC")
##### AVG accuracy for Linear Probability Model #4 built as a LPM model with y=viral
lpm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m4 = lm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m4, on_test) #type='response'#)
yhat_test = ifelse(pred_test> 0.5,1 ,0)
1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
###confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
#### Libraries
library(tidyverse)
library(mosaic)
library(class)
library(FNN)
library(foreach)
library(class)
library(jtools)
library(knitr)
library(margins)
###Data reading
on = read.csv('online_news.csv')
###on = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv")
###### Adding Viral Binary Variable and Log Shares Variable to the dataset
on$viral = ifelse(on$shares > 1400, 1, 0)
summary(on$viral)
on$lshares =log(on$shares)
##### AVG accuracy for Linear Probability Model #4 built as a LPM model with y=viral
lpm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m4 = lm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m4, on_test) #type='response'#)
yhat_test = ifelse(pred_test> 0.5,1 ,0)
1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
###confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
ACC_m4=colMeans(lpm_viral)
##### AVG accuracy for logistic Model #5 built as a logit model with y=viral
glm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m5 = glm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train, family = binomial)
pred_test = predict(m5, on_test,type='response')
yhat_test = ifelse(pred_test> 0.5,1 ,0)
###1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM1=colMeans(glm_viral)
tp1=CM1[4]
tn1=CM1[1]
fp1=CM1[3]
fn1=CM1[2]
tpr1 = tp1 / (tp1 + fn1)
tpr1
fpr1 = fp1 / (fp1 + tn1)
fpr1
ACC_m5 = (tp1+tn1)/(tp1+tn1+fp1+fn1)
ACC_m5
ERR_m5=1-ACC_m5
ERR_m5
########## AVG accuracy for Model #6 built as a KNN model with y=viral
###Define subsample
X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs,
num_self_hrefs, num_imgs, num_videos,
average_token_length, num_keywords, self_reference_avg_sharess)
y = on$viral
k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
knn_viral = do(1)*{
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train
train_ind = sample.int(n, n_train)
X_train = X[train_ind,]
X_test = X[-train_ind,]
y_train = y[train_ind]
y_test = y[-train_ind]
# scale the training set features
scale_factors = apply(X_train, 2, sd)
X_train_sc = scale(X_train, scale=scale_factors)
# scale the test set features using the same scale factors
X_test_sc = scale(X_test, scale=scale_factors)
# Fit two KNN models (notice the odd values of K)
knn_m5 = class::knn(train=X_train_sc, test= X_test_sc, cl=y_train, k=k)
# Calculating classification errors
sum(knn_m5 != y_test)/n_test
}
colMeans(knn_viral)
}
acc_k6=1-err_grid
ACC_km6=max(acc_k6)
knngraph <- data.frame(K=c(k_grid), acc=c(acc_k6))
g2=ACC_m5-ACC_m3
### AVG Accuracy model # 0 Null model for comparison (Accuraccy)
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
null_m=table(on_train$viral)
null_m
ACC_null=null_m[1]/count(on_train)
ACC_null
##### AVG accuracy for Linear Model #1 built manually
lm_shares = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
y_test   = on$viral[test_cases]
m1 = lm(shares ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m1, on_test)
yhat_test = ifelse((pred_test>1400),yes=1,no=0)
1-sum(yhat_test != y_test)/n_test
}
ACC_m1=colMeans(lm_shares)
##### AVG accuracy for Linear Model #2 built using KNN with shares
X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs,
num_self_hrefs, num_imgs, num_videos,
average_token_length, num_keywords, self_reference_avg_sharess)
y = on$shares
k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
knn_shares = do(1)*{
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train
train_ind = sample.int(n, n_train)
X_train = X[train_ind,]
X_test = X[-train_ind,]
y_train = y[train_ind]
y_test = y[-train_ind]
y_test_valid=on$viral[-train_ind]
# scale the training set features
scale_factors = apply(X_train, 2, sd)
X_train_sc = scale(X_train, scale=scale_factors)
# scale the test set features using the same scale factors
X_test_sc = scale(X_test, scale=scale_factors)
# Fit two KNN models (notice the odd values of K)
knn_m2= knn.reg(train=X_train_sc, test= X_test_sc, y=y_train, k=k)
# Calculating classification errors
phat_test = knn_m2$pred
yhat_test = ifelse((phat_test>1400),1 , 0)
sum(yhat_test != y_test_valid)/n_test
}
colMeans(knn_shares)
}
acc_k=1-err_grid
ACC_km2=max(acc_k)
ACC_km2
##### AVG accuracy for Linear Model #3 built using ln of shares
lm_lshares = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
y_test   = on$viral[test_cases]
m3 = lm(lshares ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m3, on_test)
yhat_test = ifelse(pred_test>log(1400),1 ,0)
#1-sum(yhat_test != y_test)/n_test ###Accuracy rate
confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM=colMeans(lm_lshares)
tp=CM[4]
tn=CM[1]
fp=CM[3]
fn=CM[2]
tpr = tp / (tp + fn)
tpr
fpr = fp / (fp + tn)
fpr
ACC_m3 = (tp+tn)/(tp+tn+fp+fn)
ACC_m3
ERR_m3=1-ACC_m3
ERR_m3
g1=ACC_m3-ACC_null
l1=ACC_m3/ACC_null
##### AVG accuracy for Linear Probability Model #4 built as a LPM model with y=viral
lpm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m4 = lm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m4, on_test) #type='response'#)
yhat_test = ifelse(pred_test> 0.5,1 ,0)
1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
###confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
ACC_m4=colMeans(lpm_viral)
##### AVG accuracy for logistic Model #5 built as a logit model with y=viral
glm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m5 = glm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train, family = binomial)
pred_test = predict(m5, on_test,type='response')
yhat_test = ifelse(pred_test> 0.5,1 ,0)
###1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM1=colMeans(glm_viral)
tp1=CM1[4]
tn1=CM1[1]
fp1=CM1[3]
fn1=CM1[2]
tpr1 = tp1 / (tp1 + fn1)
tpr1
fpr1 = fp1 / (fp1 + tn1)
fpr1
ACC_m5 = (tp1+tn1)/(tp1+tn1+fp1+fn1)
ACC_m5
ERR_m5=1-ACC_m5
ERR_m5
########## AVG accuracy for Model #6 built as a KNN model with y=viral
###Define subsample
X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs,
num_self_hrefs, num_imgs, num_videos,
average_token_length, num_keywords, self_reference_avg_sharess)
y = on$viral
k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
knn_viral = do(1)*{
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train
train_ind = sample.int(n, n_train)
X_train = X[train_ind,]
X_test = X[-train_ind,]
y_train = y[train_ind]
y_test = y[-train_ind]
# scale the training set features
scale_factors = apply(X_train, 2, sd)
X_train_sc = scale(X_train, scale=scale_factors)
# scale the test set features using the same scale factors
X_test_sc = scale(X_test, scale=scale_factors)
# Fit two KNN models (notice the odd values of K)
knn_m5 = class::knn(train=X_train_sc, test= X_test_sc, cl=y_train, k=k)
# Calculating classification errors
sum(knn_m5 != y_test)/n_test
}
colMeans(knn_viral)
}
acc_k6=1-err_grid
ACC_km6=max(acc_k6)
knngraph <- data.frame(K=c(k_grid), acc=c(acc_k6))
g2=ACC_m5-ACC_m3
l2=ACC_m5/ACC_m3
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col="red", label= paste ("LPM")))+
geom_line(aes(x= k, y = ACC_m5, col="green"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4)+
geom_line(aes(x= k, y = ACC_m5, col="green"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4))+
geom_line(aes(x= k, y = ACC_m5, col="green"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4))+
geom_line(aes(x= k, y = ACC_m5))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "red"))+
geom_line(aes(x= k, y = ACC_m5))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "red"))+
geom_line(aes(x= k, y = ACC_m5, col = "green"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
scale_color_discrete(name = "Model", labels = c("Y2", "Y1"))
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
scale_color_discrete(name = "Model", labels = c("LPM", "Logit"))
labs(x="K", y = "Accuracy Rate")
plotknn
install.packages("glmnet")
library(tidyverse)
library(glmnet)
train = read.csv("gft_train.csv")
train = read.csv("gft_train.csv")
getwd()
train = read.csv("gft_train.csv")
train = read.csv("gft_train.csv")
train = read.csv("gft_train")
train = read.csv("gft_train.csv")
train = read.csv("gft_train.csv")
train = read.csv("gft_train")
train = read.csv("gft_train.csv")
train = read.csv("gft_train")
