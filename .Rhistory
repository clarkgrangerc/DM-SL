knnpred = as.numeric(levels(knnmodel))[as.integer(knnmodel)]
lmmodel = lm(price ~ lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+
fireplaces + fuel + age , data=fulltrain)
lm_pred = predict(lmmodel, fulltest)
}
modelcom = cbind(ytest, lm_pred, knnpred)
}
variance$lm_sqerror = (variance$ytest-variance$lm_pred)^2
variance$knn_sqerror = (variance$ytest-variance$knnpred)^2
variance <- within(variance, Percentile <- as.integer(cut(ytest, quantile(ytest, probs=0:20/20), include.lowest=TRUE)))
var = variance %>%
group_by(Percentile)  %>%  # group the data points by model nae
summarize(lm_rmse = sqrt(mean((lm_sqerror))),
knn_rmse = sqrt(mean((knn_sqerror))))
var$Percentile= (var$Percentile*5)
variance = do(100)*{{train_ind = sample.int(n, n_train, replace = FALSE)
fulltrain = Housing[train_ind,]
Xtrain = X[train_ind,]
Xtest = X[-train_ind,]
fulltest = Housing[-train_ind,]
ytrain = y[train_ind]
ytest = y[-train_ind]
# scale the training set features
scale_factors = apply(Xtrain, 2, sd)
Xtrainsc = scale(Xtrain, scale=scale_factors)
# scale the test set features using the same scale factors
Xtestsc = scale(Xtest, scale=scale_factors)
knnmodel = knn(train=Xtrainsc, test= Xtestsc, cl=ytrain, k=120)
knnpred = as.numeric(levels(knnmodel))[as.integer(knnmodel)]
lmmodel = lm(price ~ lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+
fireplaces + fuel + age , data=fulltrain)
lm_pred = predict(lmmodel, fulltest)
}
modelcom = cbind(ytest, lm_pred, knnpred)
}
library(tidyverse)
library(mosaic)
library(ggplot2)
library(foreach)
library(class)
library(reshape2)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(foreach)
library(class)
library(reshape2)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(foreach)
library(class)
library(reshape2)
variance = do(100)*{{train_ind = sample.int(n, n_train, replace = FALSE)
fulltrain = Housing[train_ind,]
Xtrain = X[train_ind,]
Xtest = X[-train_ind,]
fulltest = Housing[-train_ind,]
ytrain = y[train_ind]
ytest = y[-train_ind]
# scale the training set features
scale_factors = apply(Xtrain, 2, sd)
Xtrainsc = scale(Xtrain, scale=scale_factors)
# scale the test set features using the same scale factors
Xtestsc = scale(Xtest, scale=scale_factors)
knnmodel = knn(train=Xtrainsc, test= Xtestsc, cl=ytrain, k=120)
knnpred = as.numeric(levels(knnmodel))[as.integer(knnmodel)]
lmmodel = lm(price ~ lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+
fireplaces + fuel + age , data=fulltrain)
lm_pred = predict(lmmodel, fulltest)
}
modelcom = cbind(ytest, lm_pred, knnpred)
}
variance$lm_sqerror = (variance$ytest-variance$lm_pred)^2
variance$knn_sqerror = (variance$ytest-variance$knnpred)^2
variance <- within(variance, Percentile <- as.integer(cut(ytest, quantile(ytest, probs=0:20/20), include.lowest=TRUE)))
var = variance %>%
group_by(Percentile)  %>%  # group the data points by model nae
summarize(lm_rmse = sqrt(mean((lm_sqerror))),
knn_rmse = sqrt(mean((knn_sqerror))))
var$Percentile= (var$Percentile*5)
vardata = melt(var, "Percentile")
ggplotly(ggplot(data=vardata, aes(x=Percentile, y=value , fill = variable)) +
geom_bar(stat="identity", position ="identity", alpha=.3 ))
library(plotly)
ggplotly(ggplot(variance)+geom_col(mapping= aes(x= ytest, y = lm_sqerror)))
ggplot(data=vardata, aes(x=Percentile, y=value , fill = variable)) +
geom_bar(stat="identity", position ="identity", alpha=.3 )
write.csv(vardata, file = "vardata.csv")
knitr::opts_chunk$set(echo = TRUE)
library(mosaic)
library(ggplot2)
library(foreach)
library(class)
library(reshape2)
library(plotly)
vardata = read.csv("vardata.csv")
ggplotly(ggplot(data=vardata, aes(x=Percentile, y=value , fill = variable)) +
geom_bar(stat="identity", position ="identity", alpha=.3 )+
labs(title="RMSE of Prediction at Different Percentiles of Prices",
subtitle = "Bin width = 5 percentile",
x="Prices Percentile",
y = "Mean RMSE")+
theme(
plot.title = element_text(hjust = 0.5) ))
vardata = read.csv("vardata.csv")
ggplotly(ggplot(data=vardata, aes(x=Percentile, y=value , fill = variable)) +
geom_bar(stat="identity", position ="identity", alpha=.3 )+
labs(title="RMSE of Prediction at Different Percentiles of Prices",
caption = "Bin width = 5 percentile",
x="Prices Percentile",
y = "Mean RMSE")+
theme(
plot.title = element_text(hjust = 0.5) ))
variance <- within(variance, Percentile <- as.integer(cut(ytest, quantile(ytest, probs=0:20/20), include.lowest=TRUE)))
var = variance %>%
group_by(Percentile)  %>%  # group the data points by model nae
summarize(lm_rmse = sqrt(mean((lm_sqerror))),
knn_rmse = sqrt(mean((knn_sqerror))))
var$Percentile= (var$Percentile*5)-5
vardata = melt(var, "Percentile")
ggplot(data=vardata, aes(x=Percentile, y=value , fill = variable)) +
geom_bar(stat="identity", position ="identity", alpha=.3 )
write.csv(vardata, file = "vardata.csv")
vardata = read.csv("vardata.csv")
ggplotly(ggplot(data=vardata, aes(x=Percentile, y=value , fill = variable)) +
geom_bar(stat="identity", position ="identity", alpha=.3 )+
labs(title="RMSE of Prediction at Different Percentiles of Prices",
caption = "Bin width = 5 percentile",
x="Prices Percentile",
y = "Mean RMSE")+
theme(
plot.title = element_text(hjust = 0.5) ))
Test1= train_ind = sample.int(n, n_train, replace = FALSE)
fulltrn = Housing[train_ind,]
Xtra = X[train_ind,]
Xtst = X[-train_ind,]
fulltst = Housing[-train_ind,]
ytrn = y[train_ind]
actual = y[-train_ind]
# scale the training set features
scale_factors = apply(Xtra, 2, sd)
Xtrasc = scale(Xtra, scale=scale_factors)
# scale the test set features using the same scale factors
Xtstsc = scale(Xtst, scale=scale_factors)
knnmodel = knn(train=Xtrasc, test= Xtstsc, cl=ytrn, k=135)
knnpredict = as.numeric(levels(knnmodel))[as.integer(knnmodel)]
lmmodel = lm(price ~ lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+
fireplaces + fuel + age , data=fulltrn)
lm_predict = predict(lmmodel, fulltst)
RMSE_LM = rmse(fulltst$price, lm_predict)
RMSE_Knn = rmse(actual, knnpredict)
PvAtest= melt(data.frame(cbind(actual, lm_predict, knnpredict)), "actual" )
### Predicted vs Actual Plot
ggplotly(ggplot(data= PvAtest)+ geom_point(mapping = aes(x=actual, y= value, color= variable), alpha = 0.3)+
geom_abline(intercept= 0, slope=1,)+
labs(title="Actual vs Predicted Plot",
caption = "Plot for both LM and KNN model",
x="Actual Prices",
y = "Predicted Prices")+
theme(
plot.title = element_text(hjust = 0.5)))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(foreach)
library(class)
library(reshape2)
library(plotly)
Housing= read.csv("Housing.csv")
Housing$extrarooms = Housing$rooms-Housing$bedrooms
Housing$C_air= ifelse(Housing$centralAir == "Yes", 1, 0)
Housing$new_c= ifelse(Housing$newConstruction == "Yes", 1, 0)
Housing$heating_electric = ifelse(Housing$heating == "electric", 1, 0)
Housing$heating_hotair = ifelse(Housing$heating == "hot air", 1, 0)
Housing$heating_watersteam = ifelse(Housing$heating == "hot water/steam", 1, 0)
n = nrow(Housing)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
rmse = function(y, yhat) {
sqrt( mean( (y - yhat)^2 ) )}
Mean_medium = do(1000)*{
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
Housing_train = Housing[train_cases,]
Housing_test = Housing[test_cases,]
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms +
fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=Housing_train)
yhat_test = predict(lm_medium, Housing_test)
rmse(Housing_test$price, yhat_test)}
library(tidyverse)
Mean_rmse = do(1000)*{
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
Housing_train = Housing[train_cases,]
Housing_test = Housing[test_cases,]
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+
fireplaces + fuel + age+ pctCollege , data=Housing_train)
### Option 1 Livingperlotsize + lotSize+ centralAir + heating + extrarooms +
## age + bedrooms +  bathperbed , data = Housing)
yhat_test = predict(lmbase, Housing_test)
rmse(Housing_test$price, yhat_test)}
mean(Mean_rmse$result, na.rm = TRUE)
library(mosaic)
Mean_rmse = do(1000)*{
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
Housing_train = Housing[train_cases,]
Housing_test = Housing[test_cases,]
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+
fireplaces + fuel + age+ pctCollege , data=Housing_train)
### Option 1 Livingperlotsize + lotSize+ centralAir + heating + extrarooms +
## age + bedrooms +  bathperbed , data = Housing)
yhat_test = predict(lmbase, Housing_test)
rmse(Housing_test$price, yhat_test)}
mean(Mean_rmse$result, na.rm = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(foreach)
library(class)
library(plotly)
Housing= read.csv("Housing.csv")
Housing$extrarooms = Housing$rooms-Housing$bedrooms
Housing$C_air= ifelse(Housing$centralAir == "Yes", 1, 0)
Housing$new_c= ifelse(Housing$newConstruction == "Yes", 1, 0)
Housing$heating_electric = ifelse(Housing$heating == "electric", 1, 0)
Housing$heating_hotair = ifelse(Housing$heating == "hot air", 1, 0)
Housing$heating_watersteam = ifelse(Housing$heating == "hot water/steam", 1, 0)
n = nrow(Housing)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
rmse = function(y, yhat) {
sqrt( mean( (y - yhat)^2 ) )}
Mean_medium = do(1000)*{
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
Housing_train = Housing[train_cases,]
Housing_test = Housing[test_cases,]
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms +
fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=Housing_train)
yhat_test = predict(lm_medium, Housing_test)
rmse(Housing_test$price, yhat_test)}
Mean_medium = do(1000)*{
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
Housing_train = Housing[train_cases,]
Housing_test = Housing[test_cases,]
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms +
fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=Housing_train)
yhat_test = predict(lm_medium, Housing_test)
rmse(Housing_test$price, yhat_test)}
mean(Mean_medium$result, na.rm = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(class)
library(reshape2)
library(pander)
#### Libraries
library(tidyverse)
library(mosaic)
library(class)
library(FNN)
library(foreach)
library(class)
library(jtools)
library(knitr)
library(margins)
###Data reading
on = read.csv('online_news.csv')
###on = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv")
###### Adding Viral Binary Variable and Log Shares Variable to the dataset
on$viral = ifelse(on$shares > 1400, 1, 0)
summary(on$viral)
on$lshares =log(on$shares)
### AVG Accuracy model # 0 Null model for comparison (Accuraccy)
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
null_m=table(on_train$viral)
null_m
ACC_null=null_m[1]/count(on_train)
ACC_null
##### AVG accuracy for Linear Model #1 built manually
lm_shares = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
y_test   = on$viral[test_cases]
m1 = lm(shares ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m1, on_test)
yhat_test = ifelse((pred_test>1400),yes=1,no=0)
1-sum(yhat_test != y_test)/n_test
}
ACC_m1=colMeans(lm_shares)
##### AVG accuracy for Linear Model #2 built using KNN with shares
X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs,
num_self_hrefs, num_imgs, num_videos,
average_token_length, num_keywords, self_reference_avg_sharess)
y = on$shares
k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
knn_shares = do(1)*{
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train
train_ind = sample.int(n, n_train)
X_train = X[train_ind,]
X_test = X[-train_ind,]
y_train = y[train_ind]
y_test = y[-train_ind]
y_test_valid=on$viral[-train_ind]
# scale the training set features
scale_factors = apply(X_train, 2, sd)
X_train_sc = scale(X_train, scale=scale_factors)
# scale the test set features using the same scale factors
X_test_sc = scale(X_test, scale=scale_factors)
# Fit two KNN models (notice the odd values of K)
knn_m2= knn.reg(train=X_train_sc, test= X_test_sc, y=y_train, k=k)
# Calculating classification errors
phat_test = knn_m2$pred
yhat_test = ifelse((phat_test>1400),1 , 0)
sum(yhat_test != y_test_valid)/n_test
}
colMeans(knn_shares)
}
acc_k=1-err_grid
ACC_km2=max(acc_k)
ACC_km2
##### AVG accuracy for Linear Model #3 built using ln of shares
lm_lshares = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
y_test   = on$viral[test_cases]
m3 = lm(lshares ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m3, on_test)
yhat_test = ifelse(pred_test>log(1400),1 ,0)
#1-sum(yhat_test != y_test)/n_test ###Accuracy rate
confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM=colMeans(lm_lshares)
tp=CM[4]
tn=CM[1]
fp=CM[3]
fn=CM[2]
tpr = tp / (tp + fn)
tpr
fpr = fp / (fp + tn)
fpr
ACC_m3 = (tp+tn)/(tp+tn+fp+fn)
ACC_m3
ERR_m3=1-ACC_m3
ERR_m3
g1=ACC_m3-ACC_null
l1=ACC_m3/ACC_null
##### AVG accuracy for Linear Probability Model #4 built as a LPM model with y=viral
lpm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m4 = lm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m4, on_test) #type='response'#)
yhat_test = ifelse(pred_test> 0.5,1 ,0)
1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
###confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
ACC_m4=colMeans(lpm_viral)
##### AVG accuracy for logistic Model #5 built as a logit model with y=viral
glm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m5 = glm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train, family = binomial)
pred_test = predict(m5, on_test,type='response')
yhat_test = ifelse(pred_test> 0.5,1 ,0)
###1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM1=colMeans(glm_viral)
tp1=CM1[4]
tn1=CM1[1]
fp1=CM1[3]
fn1=CM1[2]
tpr1 = tp1 / (tp1 + fn1)
tpr1
fpr1 = fp1 / (fp1 + tn1)
fpr1
ACC_m5 = (tp1+tn1)/(tp1+tn1+fp1+fn1)
ACC_m5
ERR_m5=1-ACC_m5
ERR_m5
########## AVG accuracy for Model #6 built as a KNN model with y=viral
###Define subsample
X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs,
num_self_hrefs, num_imgs, num_videos,
average_token_length, num_keywords, self_reference_avg_sharess)
y = on$viral
k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
knn_viral = do(1)*{
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train
train_ind = sample.int(n, n_train)
X_train = X[train_ind,]
X_test = X[-train_ind,]
y_train = y[train_ind]
y_test = y[-train_ind]
# scale the training set features
scale_factors = apply(X_train, 2, sd)
X_train_sc = scale(X_train, scale=scale_factors)
# scale the test set features using the same scale factors
X_test_sc = scale(X_test, scale=scale_factors)
# Fit two KNN models (notice the odd values of K)
knn_m5 = class::knn(train=X_train_sc, test= X_test_sc, cl=y_train, k=k)
# Calculating classification errors
sum(knn_m5 != y_test)/n_test
}
colMeans(knn_viral)
}
acc_k6=1-err_grid
ACC_km6=max(acc_k6)
knngraph <- data.frame(K=c(k_grid), acc=c(acc_k6))
g2=ACC_m5-ACC_m3
l2=ACC_m5/ACC_m3
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
scale_color_discrete(name = "Model", labels = c("LPM", "Logit"))
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
scale_color_discrete(name = "Model", labels = c("LPM", "Logit"))
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
scale_color_discrete(name = "Model", labels = c("LPM", "Logit"))+
labs(x="K", y = "Accuracy Rate")
plotknn
