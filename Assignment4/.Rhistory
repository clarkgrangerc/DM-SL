View(data)
#data$lcases=log(data$cases)
data$Total_Population=log(data$Total_Population)
data$Housing_units=log(data$Housing_units)
data$Median_Household_Income=log(data$Median_Household_Income)
data$Household_Income=log(data$Household_Income)
x<- data[-c(1:3,5,83)]
rmse_tree = do(10)*{
# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
windows()
plot(cvm1)
text(cvm1)
yhat_test1 = predict(cvm1, x_test)
c(rmse(x_test$cases,yhat_test1),length.btree)
}
info.trees=colMeans(rmse_tree)
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
windows()
prp(cvm1)
######################################
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m2 = randomForest(cases ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
rmse(x_test$cases,yhat_test2)
windows()
varImpPlot(m2,type=2)
data=read.csv("covid2zargham.csv")
data=read.csv("covid2zargham.csv")
names=read.table("names.txt")
cnames=names[,1]
colnames(data)=cnames
View(data)
View(data)
data=read.csv("covid2.csv")
names=read.table("names.txt")
cnames=names[,1]
colnames(data)=cnames
View(data)
View(data)
x<- data[-c(1:3,5)]
############################
###### 3.A. Tree model #####
rmse_tree = do(10)*{
# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
windows()
plot(cvm1)
text(cvm1)
yhat_test1 = predict(cvm1, x_test)
c(rmse(x_test$cases,yhat_test1),length.btree)
}
info.trees=colMeans(rmse_tree)
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
############################
###### 3.A. Tree model #####
rmse_tree = do(10)*{
# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
windows()
plot(cvm1)
text(cvm1)
yhat_test1 = predict(cvm1, x_test)
c(rmse(x_test$cases,yhat_test1),length.btree)
}
info.trees=colMeans(rmse_tree)
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m2 = randomForest(cases ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
rmse(x_test$cases,yhat_test2)
m2 = randomForest(cases ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
m2 = randomForest(cases ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
data=read.csv("covid2zargham.csv")
names=read.table("names.txt")
cnames=names[,1]
colnames(data)=cnames
#### Create the subsample for training an testing ####
x<- data[-c(1:3,5)]
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
rmse_tree = do(10)*{
# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
windows()
plot(cvm1)
text(cvm1)
yhat_test1 = predict(cvm1, x_test)
c(rmse(x_test$cases,yhat_test1),length.btree)
}
info.trees=colMeans(rmse_tree)
windows()
prp(cvm1)
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m2 = randomForest(cases ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
rmse(x_test$cases,yhat_test2)
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m2 = randomForest(cases ~ .,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
rmse(x_test$cases,yhat_test2)
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m2 = randomForest(cases ~ .,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
rmse(x_test$cases,yhat_test2)
windows()
varImpPlot(m2,type=2)
m3 = gbm(cases ~ .,data = x_train,n.trees=5000, shrinkage=.05)
yhat_test3= predict(m3, x_test, n.trees=5000)
rmse(x_test$cases,yhat_test3)
summary(m3)
plot(m3)
m3 = gbm(cases ~ .,data = x_train,n.trees=5000, shrinkage=.05)
yhat_test3= predict(m3, x_test, n.trees=5000)
rmse(x_test$cases,yhat_test3)
summary(m3)
plot(m3)
rmse_tree = do(10)*{
# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
windows()
plot(cvm1)
text(cvm1)
yhat_test1 = predict(cvm1, x_test)
c(rmse(x_test$cases,yhat_test1),length.btree)
}
info.trees=colMeans(rmse_tree)
rmse_tree = do(10)*{
# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
yhat_test1 = predict(cvm1, x_test)
c(rmse(x_test$cases,yhat_test1),length.btree)
}
info.trees=colMeans(rmse_tree)
info.trees
windows()
prp(cvm1)
data=read.csv("covid2zargham.csv")
View(data)
View(data)
names=read.table("names.txt")
data=read.csv("covid2zargham.csv")
names=read.table("names.txt")
cnames=names[,1]
colnames(data)=cnames
x<- data[-c(1:5,35)]
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
############################
###### 3.A. Tree model #####
rmse_tree = do(10)*{
# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases100k ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
yhat_test1 = predict(cvm1, x_test)
c(rmse(x_test$cases100k,yhat_test1),length.btree)
}
info.trees=colMeans(rmse_tree)
windows()
prp(cvm1)
######################################
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m2 = randomForest(cases100k ~ .,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
rmse(x_test$cases100k,yhat_test2)
windows()
varImpPlot(m2,type=2)
m3 = gbm(cases100k ~ .,data = x_train,n.trees=5000, shrinkage=.05)
yhat_test3= predict(m3, x_test, n.trees=5000)
rmse(x_test$cases100k,yhat_test3)
summary(m3)
plot(m3)
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m1= rpart(cases100k ~ ., method="anova",data=x_train,
control=rpart.control(minsplit=5, cp=1e-6, xval=10))
nm1 = length(unique(m1$where))
# calculate the cv error + 1 standard error
# the minimum serves as our threshold
err1se = m1$cptable[,'xerror'] + m1$cptable[,'xstd']
errth = min(err1se)
# now find the largest simplest tree that beats this threshold
m1$cptable[,'xerror'] - errth
stree=min(which(m1$cptable[,'xerror'] - errth < 0))
bestm1 = m1$cptable[stree,'CP']
###### Cross validation tree model ####
cvm1 = prune(m1, cp=bestm1)
length.btree=length(unique(cvm1$where))
yhat_test1 = predict(cvm1, x_test)
windows()
prp(cvm1)
rmse(x_test$cases100k,yhat_test3)
m3 = gbm(cases100k ~ (.)^2,data = x_train,n.trees=5000, shrinkage=.05)
yhat_test3= predict(m3, x_test, n.trees=5000)
rmse(x_test$cases100k,yhat_test2)
rmse(x_test$cases100k,yhat_test3)
m2 = randomForest(cases100k ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
rmse(x_test$cases100k,yhat_test2)
windows()
varImpPlot(m2,type=2)
windows()
importance(r2, type=1, scale = F)
windows()
importance(m2, type=1, scale = F)
m2 = randomForest(cases100k ~ (.)^2,data= x_train, mtry = 5, ntree=1000,importance=T)
windows()
importance(m2, type=1, scale = F)
varimp=as.data.frame(varImp(m1))
varImpPlot(m2,type=2)
importance(m2, type=1, scale = F)
importance(m3, type=1, scale = F)
library(randomForest)
importance(m2)
randomForest::importance(m2)
windows()
randomForest::importance(m2)
windows()
randomForest::importance(m2,type=1, scale=T)
plot(randomForest::importance(m2,type=1, scale=T))
windows()
plot(randomForest::importance(m2, scale=T))
randomForest::importance(m2, scale=T)
varImpPlot(m2,type=1)
varImpPlot(m2,type=2)
m2 = randomForest(cases100k ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
windows()
varImpPlot(m2,type=2)
randomForest::importance(m2, scale=T)
randomForest::importance(m2,type=1, scale=T)
m2 = randomForest(cases100k ~ (.)^2,data= x_train, mtry = 5, ntree=1000,importance=T)
barplot(i1, main="Variable Importance", horiz=TRUE)
i1=randomForest::importance(m2,type=1, scale=T)
barplot(i1, main="Variable Importance", horiz=TRUE)
i1=table(randomForest::importance(m2,type=1, scale=T))
i1
i1=randomForest::importance(m2,type=1, scale=T)
View(i1)
View(i1)
varimp(m2)
i1=randomForest::importance(m2,type=1)
i1
install.packages("adabag")
library(adabag)
importanceplot(m3)
importanceplot(m2)
importanceplot(m2)
importanceplot(m2)
importanceplot(m2)
windows()
summary(m3)
m3 = gbm(cases100k ~ .,data = x_train,n.trees=5000, shrinkage=.05)
windows()
summary(m3)
varImpPlot(m3)
summary(m3)
tab1=summary(m3)
tab1
View(tab1)
View(tab1)
######################################
############ Random Forest ###########
######################################
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m2 = randomForest(cases100k ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
rmse(x_test$cases100k,yhat_test2)
graph1=varImpPlot(m2,type=2,main="")
varimp=as.data.frame(varImp(m1))
######################################
############ Random Forest ###########
######################################
n = nrow(x)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
x_train = x[train_cases,]
x_test = x[test_cases,]
m2 = randomForest(cases100k ~ (.)^2,data= x_train, mtry = 5, ntree=1000)
plot(m2)
yhat_test2 = predict(m2, x_test)
r1=rmse(x_test$cases100k,yhat_test2)
graph1=varImpPlot(m2,type=2,main="")
graph1
varImpPlot(m2,type=2,main="")
summary(m3)
tab1=summary(m3)[,2]
tab1
summary(m3)
plot(m3, i.var = 'Stay_home_order')
plot(m3, i.var = 1)
plot(m3, i.var = 2)
plot(m3, i.var = 3)
plot(m3, i.var = 4)
plot(m3, i.var = 5)
partialPlot(m2, x.var = "1")
install.packages("plotmo")
library(plotmo)
library(plotmo)
plotmo(m2, pmethod="partdep", all1=TRUE, all2=TRUE)
plotmo(m2, pmethod="partdep", degree1=c("Test_per_100k"))
plot(m3, i.var = 5)
partialPlot(m2, x_train ,x.var = "1")
View(x_test)
View(x_test)
partialPlot(m2, x_train[,1:30] ,x.var = "1")
partialPlot(m2, x_train[,1:30],x.var = "Stay_home_order")
windows()
par(mfrow = c(2,2))
plot(m3, i.var = 1)
plot(m3, i.var = 2)
plot(m3, i.var = 3)
plot(m3, i.var = 4)
par(mfrow = c(2,2))
plot(m3, i.var = 1)
plot(m3, i.var = 2)
plot(m3, i.var = 3)
plot(m3, i.var = 4)
