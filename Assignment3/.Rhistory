}
colMeans(knn_viral)
}
acc_k6=1-err_grid
ACC_km6=max(acc_k6)
knngraph <- data.frame(K=c(k_grid), acc=c(acc_k6))
g2=ACC_m5-ACC_m3
### AVG Accuracy model # 0 Null model for comparison (Accuraccy)
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
null_m=table(on_train$viral)
null_m
ACC_null=null_m[1]/count(on_train)
ACC_null
##### AVG accuracy for Linear Model #1 built manually
lm_shares = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
y_test   = on$viral[test_cases]
m1 = lm(shares ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m1, on_test)
yhat_test = ifelse((pred_test>1400),yes=1,no=0)
1-sum(yhat_test != y_test)/n_test
}
ACC_m1=colMeans(lm_shares)
##### AVG accuracy for Linear Model #2 built using KNN with shares
X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs,
num_self_hrefs, num_imgs, num_videos,
average_token_length, num_keywords, self_reference_avg_sharess)
y = on$shares
k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
knn_shares = do(1)*{
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train
train_ind = sample.int(n, n_train)
X_train = X[train_ind,]
X_test = X[-train_ind,]
y_train = y[train_ind]
y_test = y[-train_ind]
y_test_valid=on$viral[-train_ind]
# scale the training set features
scale_factors = apply(X_train, 2, sd)
X_train_sc = scale(X_train, scale=scale_factors)
# scale the test set features using the same scale factors
X_test_sc = scale(X_test, scale=scale_factors)
# Fit two KNN models (notice the odd values of K)
knn_m2= knn.reg(train=X_train_sc, test= X_test_sc, y=y_train, k=k)
# Calculating classification errors
phat_test = knn_m2$pred
yhat_test = ifelse((phat_test>1400),1 , 0)
sum(yhat_test != y_test_valid)/n_test
}
colMeans(knn_shares)
}
acc_k=1-err_grid
ACC_km2=max(acc_k)
ACC_km2
##### AVG accuracy for Linear Model #3 built using ln of shares
lm_lshares = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
y_test   = on$viral[test_cases]
m3 = lm(lshares ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m3, on_test)
yhat_test = ifelse(pred_test>log(1400),1 ,0)
#1-sum(yhat_test != y_test)/n_test ###Accuracy rate
confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM=colMeans(lm_lshares)
tp=CM[4]
tn=CM[1]
fp=CM[3]
fn=CM[2]
tpr = tp / (tp + fn)
tpr
fpr = fp / (fp + tn)
fpr
ACC_m3 = (tp+tn)/(tp+tn+fp+fn)
ACC_m3
ERR_m3=1-ACC_m3
ERR_m3
g1=ACC_m3-ACC_null
l1=ACC_m3/ACC_null
##### AVG accuracy for Linear Probability Model #4 built as a LPM model with y=viral
lpm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m4 = lm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
pred_test = predict(m4, on_test) #type='response'#)
yhat_test = ifelse(pred_test> 0.5,1 ,0)
1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
###confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
ACC_m4=colMeans(lpm_viral)
##### AVG accuracy for logistic Model #5 built as a logit model with y=viral
glm_viral = do(100)*{
#	resample data with replacement
# Split into training and testing sets
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
on_test = on[test_cases,]
m5 = glm(viral ~ n_tokens_title + n_tokens_content + num_hrefs +
num_self_hrefs + num_imgs + num_videos +
average_token_length + num_keywords + data_channel_is_lifestyle +
data_channel_is_entertainment + data_channel_is_bus +
data_channel_is_socmed + data_channel_is_tech +
data_channel_is_world + self_reference_avg_sharess +
is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train, family = binomial)
pred_test = predict(m5, on_test,type='response')
yhat_test = ifelse(pred_test> 0.5,1 ,0)
###1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM1=colMeans(glm_viral)
tp1=CM1[4]
tn1=CM1[1]
fp1=CM1[3]
fn1=CM1[2]
tpr1 = tp1 / (tp1 + fn1)
tpr1
fpr1 = fp1 / (fp1 + tn1)
fpr1
ACC_m5 = (tp1+tn1)/(tp1+tn1+fp1+fn1)
ACC_m5
ERR_m5=1-ACC_m5
ERR_m5
########## AVG accuracy for Model #6 built as a KNN model with y=viral
###Define subsample
X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs,
num_self_hrefs, num_imgs, num_videos,
average_token_length, num_keywords, self_reference_avg_sharess)
y = on$viral
k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
knn_viral = do(1)*{
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train
train_ind = sample.int(n, n_train)
X_train = X[train_ind,]
X_test = X[-train_ind,]
y_train = y[train_ind]
y_test = y[-train_ind]
# scale the training set features
scale_factors = apply(X_train, 2, sd)
X_train_sc = scale(X_train, scale=scale_factors)
# scale the test set features using the same scale factors
X_test_sc = scale(X_test, scale=scale_factors)
# Fit two KNN models (notice the odd values of K)
knn_m5 = class::knn(train=X_train_sc, test= X_test_sc, cl=y_train, k=k)
# Calculating classification errors
sum(knn_m5 != y_test)/n_test
}
colMeans(knn_viral)
}
acc_k6=1-err_grid
ACC_km6=max(acc_k6)
knngraph <- data.frame(K=c(k_grid), acc=c(acc_k6))
g2=ACC_m5-ACC_m3
l2=ACC_m5/ACC_m3
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col="red", label= paste ("LPM")))+
geom_line(aes(x= k, y = ACC_m5, col="green"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4)+
geom_line(aes(x= k, y = ACC_m5, col="green"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4))+
geom_line(aes(x= k, y = ACC_m5, col="green"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4))+
geom_line(aes(x= k, y = ACC_m5))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "red"))+
geom_line(aes(x= k, y = ACC_m5))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "red"))+
geom_line(aes(x= k, y = ACC_m5, col = "green"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
scale_color_discrete(name = "Model", labels = c("Y2", "Y1"))
labs(x="K", y = "Accuracy Rate")
plotknn
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
###########Graph ACC vs k ################
plotknn= ggplot(data = knngraph)+
geom_line(aes(x = k, y = AVG_ACC))+
geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
geom_line(aes(x= k, y = ACC_m4, col= "green"))+
geom_line(aes(x= k, y = ACC_m5, col = "red"))+
scale_color_discrete(name = "Model", labels = c("LPM", "Logit"))
labs(x="K", y = "Accuracy Rate")
plotknn
install.packages("glmnet")
library(tidyverse)
library(glmnet)
train = read.csv("gft_train.csv")
train = read.csv("gft_train.csv")
getwd()
train = read.csv("gft_train.csv")
train = read.csv("gft_train.csv")
train = read.csv("gft_train")
train = read.csv("gft_train.csv")
train = read.csv("gft_train.csv")
train = read.csv("gft_train")
train = read.csv("gft_train.csv")
train = read.csv("gft_train")
library(ggplot2)
library(tidyverse)
library(dplyr)
library(foreach)
library(mosaic)
library(LICORS)
library(factoextra)
library(reshape2)
setwd("~/GitHub/DM-SL/Assignment3")
rm(list = ls(all.names = TRUE))
wine= read.csv("wine.csv")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
qplot(total.sulfur.dioxide, chlorides, data=wine, color=factor(clust1$cluster))
qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(clust1$cluster))
qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(clust1$cluster), shape = factor(wine$color))
qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(wine$color), shape = factor(clust1$cluster))
qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(wine$color), shape = factor(clust1$cluster), ggtitle("Cluster Plot with Wine Color and Differentiating features"))
qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(wine$color), shape = factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")
qplot(chlorides,total.sulfur.dioxide, data=wine, color=factor(wine$color), shape = factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")
qplot(total.sulfur.dioxide,chlorides, data=wine, color=factor(wine$color), shape = factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(foreach)
library(mosaic)
library(LICORS)
library(pander)
library(reshape2)
wine= read.csv("wine.csv")
qplot(total.sulfur.dioxide,chlorides, data=wine, color=factor(wine$color), shape = factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")
Y = wine[, -c(12,13,14)]
wine_pc = prcomp(Y, scale.=TRUE)
fviz_screeplot(wine_pc, addlabels = TRUE, ylim = c(0, 35), main = "% of Variation Explained by each PCAs")
fviz_pca_var(wine_pc, col.var="contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE, title = "Variables Contribution in first two PCAs " )
wine1 = mutate(wine, QL = ifelse(quality > 5, "High", "Low"))
pander(table(wine$QL, clust1$cluster))
wine1 = mutate(wine, QL = ifelse(quality > 5, "High", "Low"))
pander(table(wine1$QL, clust1$cluster))
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(foreach)
library(mosaic)
library(LICORS)
library(pander)
library(reshape2)
library(factoextra)
wine= read.csv("wine.csv")
qplot(total.sulfur.dioxide,chlorides, data=wine, color=factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")
clust_ql= kmeans(X, centers=7, nstart=30)
pander(table(wine$quality, clust_ql$cluster), caption = "Wine Quality on Vertical axis vs Cluster group on Horizontal axis for kmeans clustering on original scaled data")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
library(ggplot2)
library(tidyverse)
library(dplyr)
library(foreach)
library(mosaic)
library(LICORS)
library(pander)
library(reshape2)
library(factoextra)
wine= read.csv("wine.csv")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", set.caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", set.caption = "Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", set.caption("Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering"), graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", set.caption(Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering), graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", graph.fontcolor = "blue")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", alignment= "center")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", alignment= "center")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", table.alignment = "center")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster),style = "grid", caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering", table.alignment = "center")
install.packages("kableExtra")
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
library(ggplot2)
library(tidyverse)
library(dplyr)
library(foreach)
library(mosaic)
library(LICORS)
library(kableExtra)
library(reshape2)
library(factoextra)
wine= read.csv("wine.csv")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
kable(table(wine$color, clust1$cluster), caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
kable(table(wine$color, clust1$cluster), caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")%>%
kable_styling(bootstrap_options = c("striped", "hover"))
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
kable(table(wine$color, clust1$cluster), caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
kable(table(wine$color, clust1$cluster), caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"),full_width = F, position = "center")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
kable(table(wine$color, clust1$cluster), caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"), position = "center")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
kable(table(wine$color, clust1$cluster), caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"),full_width = F, position = "center")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale = TRUE)
clust1 = kmeans(X, centers = 2, nstart = 25)
pander(table(wine$color, clust1$cluster), caption="Wine Color on Vertical axis vs Cluster group on Horizontal axis for kmeans Clustering")
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
library(ggplot2)
library(tidyverse)
library(dplyr)
library(foreach)
library(mosaic)
library(LICORS)
library(pander)
library(reshape2)
library(factoextra)
wine= read.csv("wine.csv")
qplot(total.sulfur.dioxide,chlorides, data=wine, color=factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")+ scale_color_manual(breaks = c("1", "2"), values=c("red", "grey"))
qplot(total.sulfur.dioxide,chlorides, data=wine, color=factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")+ scale_color_manual(breaks = c("1", "2"), values=c("grey", "pink"))
qplot(total.sulfur.dioxide,chlorides, data=wine, color=factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")+ scale_color_manual(breaks = c("1", "2"), values=c("grey", "light red"))
qplot(total.sulfur.dioxide,chlorides, data=wine, color=factor(clust1$cluster))+ggtitle("Cluster Plot with Wine Color and Differentiating features")+ scale_color_manual(breaks = c("1", "2"), values=c("grey", "maroon"))
source('~/GitHub/DM-SL/Assignment3/assignment3.R')
