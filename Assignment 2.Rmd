---
title: "Saratoga"
author: "Clark Granger, Zachary Carlson, Zargham Khan"
date: "3/4/2020"
output: 
      md_document:
      variant: markdown_github
        
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(class)
library(reshape2)
library(pander)
```

# Homework. 2
#### Group Members: Clark Granger, Zachary Carlson and Zargham Khan

### Question.1 : Saratoga Houses

The question provides a data set of variables associated with house prices in Saratoga. We have data for more than 1,700 houses which includes their prices, landvalue and other attributes like number of bedrooms, bathrooms, living area, lotsize etc. The task is to develop models for predicting the market prices of houses for tax authorities so that they can tax them at their market value. We use the given sample to construct two different models for this question. 

#### Handbuild Linear Regression Model
The first part of the question asks us to handbuild a linear regression model with price as dependent variable and using all other variables as independent variables. We start by assessing the medium model provided in Professor's script and check its RMSE by running it on 1000 different train/test samples. 

```{r Professors_Model, eval= F, echo = T }
Professors Medium model
lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                   fireplaces + bathrooms + rooms + heating + fuel + centralAir
```

```{r medium model, echo=FALSE, cache=TRUE}
Housing= read.csv("Housing.csv")

Housing$extrarooms = Housing$rooms-Housing$bedrooms
Housing$C_air= ifelse(Housing$centralAir == "Yes", 1, 0)
Housing$new_c= ifelse(Housing$newConstruction == "Yes", 1, 0)
Housing$heating_electric = ifelse(Housing$heating == "electric", 1, 0)
Housing$heating_hotair = ifelse(Housing$heating == "hot air", 1, 0)
Housing$heating_watersteam = ifelse(Housing$heating == "hot water/steam", 1, 0)

n = nrow(Housing)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train

rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )}


Mean_medium = do(1000)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  Housing_train = Housing[train_cases,]
  Housing_test = Housing[test_cases,]
  
  lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                   fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=Housing_train)
  
  yhat_test = predict(lm_medium, Housing_test)
  rmse(Housing_test$price, yhat_test)} 
paste("RMSE" ,"for", "Medium") 
mean(Mean_medium$result, na.rm = TRUE)

```


#### Part A
We make new variables like extrarooms = rooms - bedrooms. Also we include two variables landvalue and newConstruction which improves our RMSE. However, trying composite variables like living area per lotsize, bathrooms per bedroom and building value(subtracting landvalue from the property price) did not improve the out of sample RMSE of the model. We observed that adding more variables led to higher variance.  

#### Part B
We used Step() function to narrow down variables and interactions that can give us low variance but the lowest AIC model did not perform better at out of sample RMSE in multiple iterations. Including more interaction variables and polynomials manually and one by one also did not help. 

Looking at the co-efficients, we can say that lotsize, number of bedrooms, number of bathrooms, living area, central air, heating, fuel and land value are the most important variables in explaining the prices of houses in the sample. Some interaction variables also come out to be significant in the regression model but they do not contribute much to out of sample RMSE and in most cases increase error in out of sample prediction. 

So we decided to have the following model as our final best linear regression model for house prices.  


```{r coef, echo=F}
lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing)
panderOptions('table.split.table', 100)
pander(summary(lmbase))
```

We applied this model on 1000 random train/ test splits of our data and calculated its out of sample root mean squared error.

```{r best fit model regression, echo=F, cache=TRUE}
Mean_rmse = do(1000)*{
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  Housing_train = Housing[train_cases,]
  Housing_test = Housing[test_cases,]
  
  lmbase = lm(price ~ landValue+lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ fireplaces + fuel + age+ pctCollege , data=Housing_train)
                
  yhat_test = predict(lmbase, Housing_test)
  rmse(Housing_test$price, yhat_test)} 
paste("Mean", "RMSE" ,"for", "Best", "Linear", "Model") 
mean(Mean_rmse$result, na.rm = TRUE)

```



#### Part C KNN Model
In the third part, the question asks us to fit a K-nearest neighbor model. We select the same variables as our linear model and scale them accordingly to fit a KNN model. We did 300 loops for each K starting from 1 to 300 K. The average RMSE declines in the range of 100 to 150 K. However, exact value of K with minimum average RMSE changes with each iteration of 500 training/ test splits for each K.We selected K = 135 based on our 500 training/ tests sample splits. It gave an RMSE of 80616. 

```{r, echo=F}
k_grid = seq(75, 225, by=1)
X=dplyr::select(Housing, landValue, lotSize, livingArea, bedrooms, bathrooms, extrarooms, age, new_c, heating_watersteam, heating_hotair, heating_electric, 
                  fireplaces, age, C_air)
y=Housing$price
```


```{r Loop for getting optimum k , echo=F, eval=F}
X=dplyr::select(Housing, landValue, lotSize, livingArea, bedrooms, bathrooms, extrarooms, age, new_c, heating_watersteam, heating_hotair, heating_electric, 
                  fireplaces, age, C_air)
y=Housing$price


k_grid = seq(75, 225, by=1)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
  out = do(500)*{
    train_ind = sample.int(n, n_train, replace = FALSE)
    X_train = X[train_ind,]
    X_test = X[-train_ind,]
    y_train = y[train_ind]
    y_test = y[-train_ind]
    
    # scale the training set features
    scale_factors = apply(X_train, 2, sd)
    X_train_sc = scale(X_train, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    X_test_sc = scale(X_test, scale=scale_factors)
    
    # Fit KNN models
    knn_try = knn(train=X_train_sc, test= X_test_sc, cl=y_train, k=k)
    # Calculating errors
    ###knn_pred = data.frame(knn_try)
    knn_pred = as.numeric(levels(knn_try))[as.integer(knn_try)]
    error=rmse(y_test, knn_pred)
  }   }

knn_error=do.call(cbind.data.frame, err_grid)
colnames(knn_error) = paste("K", k_grid, sep = "")
Mean_knn_rmse=data.frame(colMeans(knn_error))
colnames(Mean_knn_rmse) = ("RMSE")

```
```{r R vs RMSE graph, echo=F}
meanrm = read.csv("meanrm.csv")
ggplot(meanrm, aes(x = k_grid, y = RMSE)) +
    geom_point()+geom_vline(xintercept= k_grid[meanrm$RMSE == min(meanrm$RMSE)] , col = "red")+
    geom_text(mapping = aes(x= 130, y = 80500, label= paste("K135")), angle = 0,)+
  labs(title="K vs Mean RMSE",
       caption = " Mean RMSE over 500 iterations for each K")+
  theme(
  plot.title = element_text(hjust = 0.5))

min(meanrm$RMSE)
```


### Report: Pricing Model Comparison
We have two models for predicting the prices of houses in Saratoga. One is the linear regression model and the other one in KNN model. Both these models have their strengths and weaknesses. The main metric for comparing these two models is to check their out of sample prediction error or RMSE. By running the model on more than 500 different train/ test samples we find out that Linear regression model has lower RMSE which means that on average linear model is predicting prices more accurately than the KNN model. 

Linear Regression model Mean RMSE = 59,536.05 
KNN Model Mean RMSE at K-135      = 80,616.88

### Single Random Train/Test Performance
We run both these models on a same train set and predict values for both these models on same test to compare their RMSE and Fit for same data points. 


```{r actual vs predicted, echo=F, warning= F}
Test1= train_ind = sample.int(n, n_train, replace = FALSE)
    fulltrn = Housing[train_ind,]
    Xtra = X[train_ind,]
    Xtst = X[-train_ind,]
    fulltst = Housing[-train_ind,]
    ytrn = y[train_ind]
    actual = y[-train_ind]
    
    # scale the training set features
    scale_factors = apply(Xtra, 2, sd)
    Xtrasc = scale(Xtra, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    Xtstsc = scale(Xtst, scale=scale_factors)
    
    knnmodel = knn(train=Xtrasc, test= Xtstsc, cl=ytrn, k=135)
    knnpredict = as.numeric(levels(knnmodel))[as.integer(knnmodel)]
    
    lmmodel = lm(price ~ lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ 
                   fireplaces + fuel + age , data=fulltrn)
    lm_predict = predict(lmmodel, fulltst)
    
    RMSE_LM = rmse(fulltst$price, lm_predict)
    RMSE_Knn = rmse(actual, knnpredict)


    
PvAtest= melt(data.frame(cbind(actual, lm_predict, knnpredict)), "actual" )   
### Predicted vs Actual Plot    
ggplot(data= PvAtest)+ geom_point(mapping = aes(x=actual, y= value, color= variable), alpha = 0.3)+
  geom_abline(intercept= 0, slope=1,)+
  labs(title="Actual vs Predicted Plot",
       caption = "Plot for both LM and KNN model",
        x="Actual Prices",
        y = "Predicted Prices")+
  theme(
  plot.title = element_text(hjust = 0.5))
```
```{r, echo = F}
paste("LM", "RMSE")
RMSE_LM
paste("KNN", "RMSE")
RMSE_Knn
```

Looking at the actual vs predicted plot we see that KNN model's predictions are more spread out than LM's model predictions. We can see that LM model's prediction are evenly distributed around the center line whereas the KNN model's predictions tend to be on the lower side of the line thus indicating on average lower prediction of prices as compared to the actual one.Here we see that LM model has better predictions with lower RMSE.  

We can also see that the predictions for higher prices are far from the actual prices for both models. This means that both models are not performing good at extreme values. We check the performance of both models on prices in lower and higher percentiles and check how their RMSE perform at the fringe. 

We run 100 train/ test random splits of the sample and run both models for every train/test case and then check for RMSE of both models at different percentile of prices. The table below shows that the KNN model has higher error for higher percentile data. That means houses with higher prices are predicted more inaccurately as compared to houses with average prices. The RMSE of LM model is also high but lower than KNN model but for lower percentiles, LM model has slightly higher RMSE than KNN, however the difference is not as stark as for higher percentile values. Here we can also prefer LM model over KNN as it also performs better at extreme values. 

Furthermore, root mean square errors for houses that have average prices are almost the same for both models. This means that both models have almost similar performance for values around the average.  

```{r percentile plot, echo=F, eval=F}

variance = do(100)*{{train_ind = sample.int(n, n_train, replace = FALSE)
    fulltrain = Housing[train_ind,]
    Xtrain = X[train_ind,]
    Xtest = X[-train_ind,]
    fulltest = Housing[-train_ind,]
    ytrain = y[train_ind]
    ytest = y[-train_ind]

    # scale the training set features
    scale_factors = apply(Xtrain, 2, sd)
    Xtrainsc = scale(Xtrain, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    Xtestsc = scale(Xtest, scale=scale_factors)
    
    knnmodel = knn(train=Xtrainsc, test= Xtestsc, cl=ytrain, k=120)
    knnpred = as.numeric(levels(knnmodel))[as.integer(knnmodel)]
    
    lmmodel = lm(price ~ lotSize+ livingArea+ bedrooms+ bathrooms+ extrarooms + centralAir + heating + age+ newConstruction+ 
                   fireplaces + fuel + age , data=fulltrain)
    lm_pred = predict(lmmodel, fulltest)
}    
    modelcom = cbind(ytest, lm_pred, knnpred)
}   
 
####  
variance$lm_sqerror = (variance$ytest-variance$lm_pred)^2
variance$knn_sqerror = (variance$ytest-variance$knnpred)^2

variance <- within(variance, Percentile <- as.integer(cut(ytest, quantile(ytest, probs=0:20/20), include.lowest=TRUE)))

var = variance %>%
  group_by(Percentile)  %>%  # group the data points by model nae
  summarize(lm_rmse = sqrt(mean((lm_sqerror))), 
            knn_rmse = sqrt(mean((knn_sqerror))))

var$Percentile= (var$Percentile*5)-5
vardata = melt(var, "Percentile")

```
```{r graph, echo=F}
vardata = read.csv("vardata.csv")

ggplot(data=vardata, aes(x=Percentile, y=value , fill = variable)) +
  geom_bar(stat="identity", position ="identity", alpha=.3 )+
    labs(title="RMSE of Prediction at Different Percentiles of Prices",
       caption = "Bin width = 5 percentile",
        x="Prices Percentile",
        y = "Mean RMSE")+
  theme(
  plot.title = element_text(hjust = 0.5) )
```

### Conclusion

We can see that Linear regression model performs better for predicting prices of houses in Saratoga. The linear model is easily interpretable and we can see which variables affect prices more. 

KNN model is a non parametric model and is known as a slow learning model where it has to train on the data everytime to make a prediction for new values. It cannot learn from the training data and cannot develop a generalize model and that is why it is more susceptible to noise in data and we saw that its errors increase for outliers. Furthermore, we cannot easily see which variables are contributing more towards price changes. With more dimensions or variables the performance of the KNN model deteriorates as compared to Linear model where adding more variables might improve prediction.  

Given the results, we can clearly say that Linear Regression model is a better model than KNN in this case. It has low error on average and also at extreme values.

***

### **Question 2: A Hospital Audit**
#### Part 1: Are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?
Here, we model recall decisions on all risk factors and the radiologist that made the decision to gauge how conservative the doctors are relative to each other.


```{r part 1}
brca = read.csv("brca.csv")
model_recall = glm(recall ~ . - cancer, data=brca, family=binomial)
pander(summary(model_recall))
```
In this output, we can see the estimates on the different radiologists, which tells us how conservative they are. Notice, our output only has four radiologists shown but there are actually 5 radiologists of interest. The fifth one(radiologist 13) is excluded and represented within the intercept. Then, the radiologist estimates in this output are just used to compare against that fifth radiologist. For example, radiologist 34 has an estimate of -.52, which corresponds to multiplying the radioligist 13 odds of recall by .6, holding all other risk factors equal, therefore this radiologist is less conservative than the baseline radiologist(the fifth radiolist not shown in the output) at the hospital. 

Analagously, radiologist 89 seems to be the most conservative with an estimate of .46 which cooresponds to multiplying the odds of recall by 1.58(again, comparing to the fifth radiologist not shown in the output), holding other risk factors constant.

In conclusion, yes, some radioligists seem to be more conservative than others. This model would suggest that if the same patient saw both radiologist 34 and radiologist 89, radiologist 89 would be about 2.6 times more likely to recall them. This is a pretty concerning figure. We would like our cross-doctor reliability to be higher. It seems logical that there should be some stable risk of cancer that should determine recall and which doctor you have shouldn't influence the decision, at least not to the magnitude we see here. Moreover, here is our radiologist conservativeness ranking

1. Radiologist 89

2. Radiologist 66 

3. Radiologist 13 

4. Radiologist 95 

5. Radiologist 34

#### Part 2: When the radiologists at this hospital interpret a mammogram to make a decision on whether to recall the patient, does the data suggest that they should be weighing some clinical risk factors more heavily than they currently are?

Let's model cancer versus recall and risk factors. "Model B"(regresses cancer on recall decision and risk factors) shouldn't be any better than "Model A"(regresses cancer on only recall decision) if doctors are using all the risk factor information to the fullest of its potential.
```{r part 2}
model_cancer = glm(cancer ~ ., data=brca, family=binomial)
pander(summary(model_cancer))
```

This result implies that Model B is better than Model A from above because the model is giving esimates to some of the risk factors even after controling for recall. If doctors were correctly weighting all risk factors in their recall decisions, the only non-zero estimate should be on the recall feature. The recall variable is a proxy for the probability of cancer since doctors want the most-at-risk patients to be further evaluated. So, the fact that there are non-zero estimates on features that aren't recall suggests the model was improved even after using the recall decisions by incorporating risk factors that shoud have already been used in recall.

There are some pretty large estimates on some of the risk factors! For example, tissue density type 4 had an estimate of 1.998 which cooresponds to being 7.37x more likely to have cancer than patients with density 1, holding all else fixed.

Family history of cancer, tissue density, and age all seem to have some extra information that could be utilized in the recall decision. Patients with tissue density 4 are 7.4x more likely to get cancer, holding recall decision fixed. Patients with family history of cancer are 1.28x more likely to get cancer than patients without history of breast cancer, holding recall decision fixed. This "incorrect" weighting of risk factors also shows up in the raw error rates below. This can be seen below in the increase in false positives and false negatives when moving from history=0 to history=1. 1.5% of the patients who weren't recalled ended up having cancer when they didn't have any family history of breast cancer. When they did have family history of breast cancer, this figure jumps to 2.75%. 84.8% of the patients who were recalled didn't end up having cancer in the case where they had no family history of breast cancer. In the case where they did have family history of breast cancer, this figure jumps to 86.2%. This reinforces the fact that we could get better recall performance from weighting tissue density 4 and family history of breast cancer more heavily in our decision to perform addition diagnostic tests or not. There are other factors with similar results but I just went over two of them.
```{r}
pander((xtabs(~cancer + recall + history, brca) %>% prop.table(margin=c(2, 3))))
pander(xtabs(~cancer + recall + density, brca) %>% prop.table(margin=c(2, 3)))
```
In conclusion, the doctors could adjust how they are currently weighting different risk factors to make more accurate evidence-based decisions about recall and reduce unneccessary diagnostic tests as well as make sure people who need diagnostic tests are getting them. Ultimately, this should also lead to better outcomes for the patients because we will catch the cancer sooner for the patients who have breast cancer.

***

## Question 3: Predicting when articles go viral

```{r include=FALSE}
 #### Libraries 
library(tidyverse)
library(mosaic)
library(class)
library(FNN)
library(foreach)
library(class)
library(jtools)
library(knitr)
library(margins)

###Data reading
on = read.csv('online_news.csv')
###on = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv")
###### Adding Viral Binary Variable and Log Shares Variable to the dataset
on$viral = ifelse(on$shares > 1400, 1, 0)
summary(on$viral)
on$lshares =log(on$shares)

```


Using the data of 39,797 online articles published by Mashable during 2013 and 2014, we will try to build a model to determine if an article goes viral or not. An article is considered viral if it was shared more than 1400 times. We have a set o features of each article such as things like how long the headline is, how long the article is, how positive or negative the "sentiment" of the article was, among other variables. Besides building the best classification model, Mashable wants to know if there is anything they can learn about how to improve an article's chance of reaching this threshold. 

To deal with this problem we are going to work with two approaches. The first approach is from the standpoint of regression. we will build three models, one linear regression, one K nearest neighbor and one  that transforms the objective variable. In this first approach the objective variable will be the number of shares. The second approach is from the standpoint of classification. In this case our objective variable will be a binary entry Viral, which is 1 if the article is viral or 0 otherwise. In both approaches, we have measures that help us to classify the model performance. 


### Approach #1 Working with Shares as objective variable
```{r include=FALSE, cache=TRUE}
### AVG Accuracy model # 0 Null model for comparison (Accuraccy)
n = nrow(on)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
on_train = on[train_cases,]
null_m=table(on_train$viral)
null_m
ACC_null=null_m[1]/count(on_train)
ACC_null

##### AVG accuracy for Linear Model #1 built manually

lm_shares = do(100)*{
  #	resample data with replacement
    # Split into training and testing sets
  n = nrow(on)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  on_train = on[train_cases,]
  on_test = on[test_cases,]
  y_test   = on$viral[test_cases]
  
    m1 = lm(shares ~ n_tokens_title + n_tokens_content + num_hrefs + 
            num_self_hrefs + num_imgs + num_videos + 
            average_token_length + num_keywords + data_channel_is_lifestyle +
            data_channel_is_entertainment + data_channel_is_bus + 
            data_channel_is_socmed + data_channel_is_tech +
            data_channel_is_world + self_reference_avg_sharess +
            is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
  
  pred_test = predict(m1, on_test)
  yhat_test = ifelse((pred_test>1400),yes=1,no=0)
  1-sum(yhat_test != y_test)/n_test 
  
}
ACC_m1=colMeans(lm_shares)

##### AVG accuracy for Linear Model #2 built using KNN with shares
X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs, 
                  num_self_hrefs, num_imgs, num_videos, 
                  average_token_length, num_keywords, self_reference_avg_sharess) 
y = on$shares

k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
  knn_shares = do(1)*{
    n = length(y)
    n_train = round(0.8*n)
    n_test = n - n_train
    train_ind = sample.int(n, n_train)
    X_train = X[train_ind,]
    X_test = X[-train_ind,]
    y_train = y[train_ind]
    y_test = y[-train_ind]
    y_test_valid=on$viral[-train_ind]
    
    # scale the training set features
    scale_factors = apply(X_train, 2, sd)
    X_train_sc = scale(X_train, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    X_test_sc = scale(X_test, scale=scale_factors)
    
    # Fit two KNN models (notice the odd values of K)
    knn_m2= knn.reg(train=X_train_sc, test= X_test_sc, y=y_train, k=k)
    
   # Calculating classification errors
    phat_test = knn_m2$pred
    yhat_test = ifelse((phat_test>1400),1 , 0)
    sum(yhat_test != y_test_valid)/n_test 
    
  } 
  colMeans(knn_shares)
}
acc_k=1-err_grid
ACC_km2=max(acc_k)
ACC_km2

##### AVG accuracy for Linear Model #3 built using ln of shares 

lm_lshares = do(100)*{
  #	resample data with replacement
  # Split into training and testing sets
  n = nrow(on)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  on_train = on[train_cases,]
  on_test = on[test_cases,]
  y_test   = on$viral[test_cases]
  
  
  m3 = lm(lshares ~ n_tokens_title + n_tokens_content + num_hrefs + 
            num_self_hrefs + num_imgs + num_videos + 
            average_token_length + num_keywords + data_channel_is_lifestyle +
            data_channel_is_entertainment + data_channel_is_bus + 
            data_channel_is_socmed + data_channel_is_tech +
            data_channel_is_world + self_reference_avg_sharess +
            is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
  
  pred_test = predict(m3, on_test)
  yhat_test = ifelse(pred_test>log(1400),1 ,0)
  #1-sum(yhat_test != y_test)/n_test ###Accuracy rate
  confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM=colMeans(lm_lshares)
tp=CM[4]
tn=CM[1]
fp=CM[3]
fn=CM[2]
tpr = tp / (tp + fn)
tpr
fpr = fp / (fp + tn)
fpr
ACC_m3 = (tp+tn)/(tp+tn+fp+fn)
ACC_m3
ERR_m3=1-ACC_m3
ERR_m3

g1=ACC_m3-ACC_null
l1=ACC_m3/ACC_null

```


In the first approach our objective variable is number of shares. To start  we analyzed all the features([see features](https://github.com/jgscott/ECO395M/blob/master/data/online_news_codes.txt)) we have to decide which of them are important to include in the model. Then we start fitting linear models with several combinations of variables. We used the step function to test if including some interactions in the model would be helpful, but ultimately we decided use a more parsimonious model, given it has a good fitting. Below are the variables we used in our linear models, we ran the first using shares as the explanatory variable and then we used log of shares as the explanatory variable. Then, we decided to try the knn model as well. For this last model, we did not consider the binary variables since they do not add too much information in this methodology. The variables used in the knn model are summarized below. 

**Specification for Linear models:**

Shares or log(Shares) ~ n_tokens_title + n_tokens_content + num_hrefs + 
            num_self_hrefs + num_imgs + num_videos + 
            average_token_length + num_keywords + data_channel_is_lifestyle +
            data_channel_is_entertainment + data_channel_is_bus + 
            data_channel_is_socmed + data_channel_is_tech +
            data_channel_is_world + self_reference_avg_sharess +
            is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity

**For KNN model the specification:** 

Shares ~ n_tokens_title, n_tokens_content, num_hrefs, 
                  num_self_hrefs, num_imgs, num_videos, 
                  average_token_length, num_keywords, self_reference_avg_sharess


Since the predictions of our model are numerical (number of shares), we needed the evaluation in terms of a binary prediction (viral or not). So, after generating predictions from our models across multiple train/test splits we summarized the accuracy rate of each model considering a threshold of 1400 shares to consider an article viral. In the table 1 we can see the accuracy ratio for our three models plus a "null" model that always predicts the articles as "not viral". 

**Table 1. Accuracy rate for models with Shares as objective variable**
```{r echo=FALSE, warning= FALSE}
accdata = c(ACC_null,ACC_m1,ACC_km2,ACC_m3)
accdata1=t(as.data.frame(accdata))
colnames(accdata1) <- c("Accuracy Rate")
row.names(accdata1) <- c("Null Model","Linear Model","KNN Model","Linear Model (Log Shares)")
table1=kable(accdata1) 
table1
```

In table 1 we can see the best accuracy is the linear model using log of shares. We can see that the linear model over shares is even worse that the null model.  We can see that the linear model of log shares has a gain in accuracy of `r g1*100`% comparing with the null model, or a lift of `r l1`. 
 

Having found our best model for the prediction of number of shares we proceed to present the Confusion matrix and the requested stats:

**Table 2. Confusion Matrix for the Linear Model of Log of Shares**
```{r echo=FALSE, warning= FALSE}
CM_m3=as.data.frame(CM)
colnames(CM_m3)<-c("Linear Model Log (Shares)")
row.names(CM_m3) <- c("True Negative","False Negative","False Positive","True Positive")
table2=kable(CM_m3)
table2
```

**The stats are the following:** 

 Overall error rate = `r ERR_m3*100`%
 
 True positive rate = `r tpr*100`%
 
 False positive rate = `r fpr*100`%
 
 
 
### Approach #2 Working with binary variable viral as objective 

In the second approach we handled this problem from the standpoint of classification. That is, we defined a binary variable viral and built our various models for directly predicting viral status as a target variable. We worked with the same specification of our first linear model, but using linear probability model regression and a logit model regression. In the case of the Knn approach we used the same variables of the knn model for shares.Table 3 summarizes the accuracy rate for the three classification models.  
```{r include=FALSE, cache=TRUE}
##### AVG accuracy for Linear Probability Model #4 built as a LPM model with y=viral

lpm_viral = do(100)*{
  #	resample data with replacement
  # Split into training and testing sets
  n = nrow(on)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  on_train = on[train_cases,]
  on_test = on[test_cases,]
  
  
  m4 = lm(viral ~ n_tokens_title + n_tokens_content + num_hrefs + 
            num_self_hrefs + num_imgs + num_videos + 
            average_token_length + num_keywords + data_channel_is_lifestyle +
            data_channel_is_entertainment + data_channel_is_bus + 
            data_channel_is_socmed + data_channel_is_tech +
            data_channel_is_world + self_reference_avg_sharess +
            is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train)
  
  pred_test = predict(m4, on_test) #type='response'#)
  yhat_test = ifelse(pred_test> 0.5,1 ,0)
  1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
  ###confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
ACC_m4=colMeans(lpm_viral)


##### AVG accuracy for logistic Model #5 built as a logit model with y=viral
glm_viral = do(100)*{
  #	resample data with replacement
  # Split into training and testing sets
  n = nrow(on)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  on_train = on[train_cases,]
  on_test = on[test_cases,]
  
    m5 = glm(viral ~ n_tokens_title + n_tokens_content + num_hrefs + 
            num_self_hrefs + num_imgs + num_videos + 
            average_token_length + num_keywords + data_channel_is_lifestyle +
            data_channel_is_entertainment + data_channel_is_bus + 
            data_channel_is_socmed + data_channel_is_tech +
            data_channel_is_world + self_reference_avg_sharess +
            is_weekend + global_rate_negative_words + global_rate_positive_words + title_subjectivity,data=on_train, family = binomial)
  
  pred_test = predict(m5, on_test,type='response')
  yhat_test = ifelse(pred_test> 0.5,1 ,0)
  ###1-sum(yhat_test != on_test$viral)/n_test ###Accuracy rate
  confusion_out = table(y = on_test$viral, yhat = yhat_test)
}
CM1=colMeans(glm_viral)
tp1=CM1[4]
tn1=CM1[1]
fp1=CM1[3]
fn1=CM1[2]
tpr1 = tp1 / (tp1 + fn1)
tpr1
fpr1 = fp1 / (fp1 + tn1)
fpr1
ACC_m5 = (tp1+tn1)/(tp1+tn1+fp1+fn1)
ACC_m5
ERR_m5=1-ACC_m5
ERR_m5


########## AVG accuracy for Model #6 built as a KNN model with y=viral
###Define subsample

X = dplyr::select(on, n_tokens_title, n_tokens_content, num_hrefs, 
                  num_self_hrefs, num_imgs, num_videos, 
                  average_token_length, num_keywords, self_reference_avg_sharess) 

y = on$viral


k_grid = seq(1, 100, by=10)
err_grid = foreach(k = k_grid,  .combine='c') %do% {
  knn_viral = do(1)*{
    n = length(y)
    n_train = round(0.8*n)
    n_test = n - n_train
    train_ind = sample.int(n, n_train)
    X_train = X[train_ind,]
    X_test = X[-train_ind,]
    y_train = y[train_ind]
    y_test = y[-train_ind]
    
    # scale the training set features
    scale_factors = apply(X_train, 2, sd)
    X_train_sc = scale(X_train, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    X_test_sc = scale(X_test, scale=scale_factors)
    
    # Fit two KNN models (notice the odd values of K)
    knn_m5 = class::knn(train=X_train_sc, test= X_test_sc, cl=y_train, k=k)
    
    # Calculating classification errors
    sum(knn_m5 != y_test)/n_test
  } 
  colMeans(knn_viral)
}

acc_k6=1-err_grid
ACC_km6=max(acc_k6)
knngraph <- data.frame(K=c(k_grid), acc=c(acc_k6))

g2=ACC_m5-ACC_m3
l2=ACC_m5/ACC_m3



```

**Table 3. Accuracy rate for models with  viral as objective variable**
```{r echo=FALSE, warning= FALSE}
  accdata2 = c(ACC_m4,ACC_m5,ACC_km6)
accdata21=as.data.frame(accdata2)
colnames(accdata21) <- c("Accuracy Rate")
row.names(accdata21) <- c("Linear Probability Model","Logit Model","KNN Classification Model")
table3=kable(accdata21) 
table3
```
 
I table 3, we can see that the model with the best accuraccy is the Logit Model, which is slightly superior to the Linear Probability model (`r ACC_m5` vs `r ACC_m4`). This result was expected since in this approach we are working directly over the binary variable, then the prediction range will have less variation. In the first case we predicted the number of shares, which have a huge range of possible outcomes. In the table 4, we present the confusion matrix for our best model and then we present the requested stats. We can see that the logistic model gives a gain of accuracy of `r g2*100`% over the lineal model of log shares, or a lift of `r l2`.


**Table 4. Confusion Matrix for the Logistic Model of Viral**
```{r echo=FALSE, warning= FALSE}
CM_m5=as.data.frame(CM1)
colnames(CM_m5)<-c("Logit Model")
row.names(CM_m5) <- c("True Negative","False Negative","False Positive","True Positive")
table4=kable(CM_m5) 
table4
```


**The stats are the following:** 

 Overall error rate = `r ERR_m5*100`%
 
 True positive rate = `r tpr1*100`%
 
 False positive rate = `r fpr1*100`%


In addition, we decided to include a graph to see the advantages in accuracy of the probabilistic models over the Knn classification model with differents values of K (see graph 1).  

**Graph 1. KNN models vs LPM and Logit Models**
```{r echo=FALSE, warning= FALSE}
colnames(knngraph) <- c("k","AVG_ACC")
k_min = knngraph$k[which.max(knngraph$AVG_ACC)]
plotknn= ggplot(data = knngraph)+
  geom_line(aes(x = k, y = AVG_ACC))+
  geom_line(aes(x =k_min , y = AVG_ACC), col = "blue")+
  geom_line(aes(x= k, y = ACC_m4, col= "green"))+
  geom_line(aes(x= k, y = ACC_m5, col = "red"))+
  scale_color_discrete(name = "Model", labels = c("LPM", "Logit"))+
  labs(x="K", y = "Accuracy Rate")
plotknn
```



### How can we increase the probability that an article goes viral?


By this point we have adressed the problem of finding the best model to predict viral with the most accuracy. But we also want to know what features would increase the probability of an article going viral or not. However, since our best model is a logistic model we know that the coefficients generated do not have a direct interpretation as a causal effect, but more like odds multipliers correlated with various features. Therefore, in table 6 we compute the average marginal effect of all the variables we included in our logit model, because they can be interpretted as the partial causal effects of each variable over the probability of an article to go viral. Thus, if we want to increase the probability to be viral we should increase the features with the higher partial effect such as writing an article about social media or releasing the article on the weekend. In the other hand we should avoid the characteristic that have a negative partial effect such as writing about entertainment or including a large number of negative words. 

**Table 5. Average Marginal Effects of the Logit Model**
```{r echo=FALSE, warning= FALSE}
margins_m5=margins(m5, type='response')
table5=kable(summary(margins_m5)) 
table5

```
